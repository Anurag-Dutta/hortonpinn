{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCGKeZ2gyuoQ"
      },
      "source": [
        "_Importing Required Libraries_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-6LN-zXiLcM",
        "outputId": "4de610a4-f8b8-4f49-c6c0-89de299ccedc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
            "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.26.4)\n",
            "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->hampel) (1.16.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.1.2 -> 24.0\n",
            "[notice] To update, run: C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "pip install hampel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "By_d9uXpaFvZ"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dropout\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "from hampel import hampel\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from math import sqrt\n",
        "from matplotlib import pyplot\n",
        "from numpy import array"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyOjBMFayuoR"
      },
      "source": [
        "## Pretraining"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5QqIY_GyuoR"
      },
      "source": [
        "The `horton_intermittency.dat` feeds the model with the dynamics of the Horton Chaotics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9dV4a8yfyuoR"
      },
      "outputs": [],
      "source": [
        "data = np.genfromtxt('horton_intermittency.dat')\n",
        "training_set = pd.DataFrame(data).reset_index(drop=True)\n",
        "training_set = training_set.iloc[:,1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7easoxByuoR"
      },
      "source": [
        "## Computing the Gradients"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SnyolJTyuoR"
      },
      "source": [
        "_Calculating the value of_ $\\frac{dx}{dt}$, $\\frac{d^2x}{dt^2}$, _and_ $\\frac{d^3x}{dt^3}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmIbVfIvyuoR",
        "outputId": "aa4e3136-c854-465d-d2e9-81cfd546e440"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.0\n",
            "1       -0.000011\n",
            "2        0.003571\n",
            "3        0.005754\n",
            "4        0.006818\n",
            "5       -0.000807\n",
            "           ...   \n",
            "9996    -0.129763\n",
            "9997    -0.118735\n",
            "9998    -0.105414\n",
            "9999    -0.090338\n",
            "10000   -0.074048\n",
            "Name: 1, Length: 10000, dtype: float64\n",
            "2        0.003582\n",
            "3        0.002183\n",
            "4        0.001064\n",
            "5       -0.007625\n",
            "6       -0.006999\n",
            "           ...   \n",
            "9996     0.008219\n",
            "9997     0.011028\n",
            "9998     0.013321\n",
            "9999     0.015076\n",
            "10000    0.016290\n",
            "Name: 1, Length: 9999, dtype: float64\n",
            "2        0.003582\n",
            "3        0.002183\n",
            "4        0.001064\n",
            "5       -0.007625\n",
            "6       -0.006999\n",
            "           ...   \n",
            "9996     0.008219\n",
            "9997     0.011028\n",
            "9998     0.013321\n",
            "9999     0.015076\n",
            "10000    0.016290\n",
            "Name: 1, Length: 9999, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "t_diff = 1\n",
        "print(training_set.max())\n",
        "gradient_t = (training_set.diff()/t_diff).iloc[1:] # dx/dt\n",
        "print(gradient_t)\n",
        "gradient_tt = (gradient_t.diff()/t_diff).iloc[1:] # d2x/dt2\n",
        "print(gradient_tt)\n",
        "gradient_ttt = (gradient_tt.diff()/t_diff).iloc[1:] # d3x/dt3\n",
        "print(gradient_tt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2eVeeoxyuoS"
      },
      "source": [
        "## Loading Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "0J-NKyIEyuoS"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0       3.013502\n",
              "1       2.832119\n",
              "2       2.042342\n",
              "3       1.794970\n",
              "4       1.789537\n",
              "          ...   \n",
              "2010    0.092451\n",
              "2011    0.097335\n",
              "2012    0.105332\n",
              "2013    0.103997\n",
              "2014    0.105429\n",
              "Name: flux, Length: 2015, dtype: float64"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = pd.read_csv(\"differential-protons-7-day.csv\")\n",
        "training_set = data.iloc[:, 1]\n",
        "training_set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-CbNUhJ74UqF",
        "outputId": "20f562d8-8247-49cc-b9c3-00eca5e13e2d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0       3.013502\n",
              "1       2.832119\n",
              "2       2.042342\n",
              "3       1.794970\n",
              "4       1.789537\n",
              "          ...   \n",
              "1959    0.156050\n",
              "1960    0.169289\n",
              "1961    0.183471\n",
              "1962    0.178418\n",
              "1963    0.190143\n",
              "Name: flux, Length: 1964, dtype: float64"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test = training_set.tail(50)\n",
        "test\n",
        "training_set = training_set.head(1964) # (2013 - 50) + 1\n",
        "training_set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0TwTcq0yuoS",
        "outputId": "37252ed8-d88e-4044-fa7a-922411990b5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0      -0.000011\n",
            "1       0.003571\n",
            "2       0.005754\n",
            "3       0.006818\n",
            "4      -0.000807\n",
            "          ...   \n",
            "9995   -0.129763\n",
            "9996   -0.118735\n",
            "9997   -0.105414\n",
            "9998   -0.090338\n",
            "9999   -0.074048\n",
            "Name: 1, Length: 10000, dtype: float64\n",
            "0       0.003582\n",
            "1       0.002183\n",
            "2       0.001064\n",
            "3      -0.007625\n",
            "4      -0.006999\n",
            "          ...   \n",
            "9994    0.008219\n",
            "9995    0.011028\n",
            "9996    0.013321\n",
            "9997    0.015076\n",
            "9998    0.016290\n",
            "Name: 1, Length: 9999, dtype: float64\n",
            "0      -0.001400\n",
            "1      -0.001118\n",
            "2      -0.008690\n",
            "3       0.000626\n",
            "4       0.000763\n",
            "          ...   \n",
            "9993    0.003290\n",
            "9994    0.002810\n",
            "9995    0.002293\n",
            "9996    0.001755\n",
            "9997    0.001214\n",
            "Name: 1, Length: 9998, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "training_set = training_set.reset_index(drop=True) # sets a list of integer ranging from 0 to length of training_set as index\n",
        "gradient_t = gradient_t.reset_index(drop=True) # sets a list of integer ranging from 0 to length of gradient_t as index\n",
        "gradient_tt = gradient_tt.reset_index(drop=True) # sets a list of integer ranging from 0 to length of gradient_tt as index\n",
        "gradient_ttt = gradient_ttt.reset_index(drop=True) # sets a list of integer ranging from 0 to length of gradient_ttt as index\n",
        "print(gradient_t)\n",
        "print(gradient_tt)\n",
        "print(gradient_ttt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "O2biznZQyuoS"
      },
      "outputs": [],
      "source": [
        "df = pd.concat((training_set, gradient_t), axis=1) ##########[:-1]\n",
        "gradient_tt.columns = [\"grad_tt\"]\n",
        "df = pd.concat((df, gradient_tt), axis=1) ################[:-1]\n",
        "gradient_tt.columns = [\"grad_ttt\"]\n",
        "df = pd.concat((df, gradient_ttt), axis=1) ################[:-1]\n",
        "df.columns = ['y_t', 'grad_t', 'grad_tt', 'grad_ttt']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "sk_a5v3tyuoS",
        "outputId": "17563625-e550-45ae-faab-fafa353e44da"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>y_t</th>\n",
              "      <th>grad_t</th>\n",
              "      <th>grad_tt</th>\n",
              "      <th>grad_ttt</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3.013502</td>\n",
              "      <td>-0.000011</td>\n",
              "      <td>0.003582</td>\n",
              "      <td>-0.001400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.832119</td>\n",
              "      <td>0.003571</td>\n",
              "      <td>0.002183</td>\n",
              "      <td>-0.001118</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2.042342</td>\n",
              "      <td>0.005754</td>\n",
              "      <td>0.001064</td>\n",
              "      <td>-0.008690</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.794970</td>\n",
              "      <td>0.006818</td>\n",
              "      <td>-0.007625</td>\n",
              "      <td>0.000626</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.789537</td>\n",
              "      <td>-0.000807</td>\n",
              "      <td>-0.006999</td>\n",
              "      <td>0.000763</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9995</th>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.129763</td>\n",
              "      <td>0.011028</td>\n",
              "      <td>0.002293</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9996</th>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.118735</td>\n",
              "      <td>0.013321</td>\n",
              "      <td>0.001755</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9997</th>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.105414</td>\n",
              "      <td>0.015076</td>\n",
              "      <td>0.001214</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9998</th>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.090338</td>\n",
              "      <td>0.016290</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9999</th>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.074048</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10000 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           y_t    grad_t   grad_tt  grad_ttt\n",
              "0     3.013502 -0.000011  0.003582 -0.001400\n",
              "1     2.832119  0.003571  0.002183 -0.001118\n",
              "2     2.042342  0.005754  0.001064 -0.008690\n",
              "3     1.794970  0.006818 -0.007625  0.000626\n",
              "4     1.789537 -0.000807 -0.006999  0.000763\n",
              "...        ...       ...       ...       ...\n",
              "9995       NaN -0.129763  0.011028  0.002293\n",
              "9996       NaN -0.118735  0.013321  0.001755\n",
              "9997       NaN -0.105414  0.015076  0.001214\n",
              "9998       NaN -0.090338  0.016290       NaN\n",
              "9999       NaN -0.074048       NaN       NaN\n",
              "\n",
              "[10000 rows x 4 columns]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df # DataFrame"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5esyHu5aFvg"
      },
      "source": [
        "## Plot of the External Forcing from Chaotic Differential Equation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "hGnE43tOh-4p",
        "outputId": "fc396503-b624-4fa5-dfbe-f460207405c6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Axes: >"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "df.iloc[:, 0].plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "ym4xWUUxaFvg",
        "outputId": "ae6a3495-8ce9-437e-ba81-3ed31deedeae"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Axes: >"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import numpy as np\n",
        "c1=2.6676\n",
        "c2=7000\n",
        "c3=20000\n",
        "b3=1.06\n",
        "L = df.iloc[:, 3] + (c1 * df.iloc[:, 1] + b3 * df.iloc[:, 2] + c2 + c3*np.tanh(df.iloc[:,0]))\n",
        "L.plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VyEywnwaFvh"
      },
      "source": [
        "## Preprocessing the data into supervised learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "6V9dXqzdaFvh"
      },
      "outputs": [],
      "source": [
        "# split a sequence into samples\n",
        "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
        "    n_vars = 1 if type(data) is list else data.shape[1]\n",
        "    df = pd.DataFrame(data)\n",
        "    cols, names = list(), list()\n",
        "    # input sequence (t-n_in, ... t-1)\n",
        "    for i in range(n_in, 0, -1):\n",
        "        cols.append(df.shift(i))\n",
        "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
        "    # forecast sequence (t, t+1, ... t+n_out)\n",
        "    for i in range(0, n_out):\n",
        "      cols.append(df.shift(-i))\n",
        "      if i == 0:\n",
        "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
        "      else:\n",
        "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
        "    # put it all together\n",
        "    agg = pd.concat(cols, axis=1)\n",
        "    agg.columns = names\n",
        "    # drop rows with NaN values\n",
        "    if dropnan:\n",
        "       agg.dropna(inplace=True)\n",
        "    return agg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CrzSrT1HnyfH",
        "outputId": "7e75f928-3e47-499d-eac1-51908015ef78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     var1(t-175)  var1(t-174)  var1(t-173)  var1(t-172)  var1(t-171)  \\\n",
            "175     3.013502     2.832119     2.042342     1.794970     1.789537   \n",
            "176     2.832119     2.042342     1.794970     1.789537     1.877289   \n",
            "177     2.042342     1.794970     1.789537     1.877289     1.983688   \n",
            "178     1.794970     1.789537     1.877289     1.983688     2.153054   \n",
            "179     1.789537     1.877289     1.983688     2.153054     2.331594   \n",
            "\n",
            "     var1(t-170)  var1(t-169)  var1(t-168)  var1(t-167)  var1(t-166)  ...  \\\n",
            "175     1.877289     1.983688     2.153054     2.331594     2.405767  ...   \n",
            "176     1.983688     2.153054     2.331594     2.405767     1.515924  ...   \n",
            "177     2.153054     2.331594     2.405767     1.515924     2.041262  ...   \n",
            "178     2.331594     2.405767     1.515924     2.041262     2.062020  ...   \n",
            "179     2.405767     1.515924     2.041262     2.062020     1.730290  ...   \n",
            "\n",
            "     var3(t+47)  var4(t+47)  var1(t+48)  var2(t+48)  var3(t+48)  var4(t+48)  \\\n",
            "175   -0.036885    0.002002    1.581564    0.034651   -0.034883    0.002773   \n",
            "176   -0.034883    0.002773    1.530042   -0.000232   -0.032110    0.003410   \n",
            "177   -0.032110    0.003410    1.391068   -0.032342   -0.028700    0.003905   \n",
            "178   -0.028700    0.003905    3.933360   -0.061043   -0.024795    0.004257   \n",
            "179   -0.024795    0.004257    5.136263   -0.085838   -0.020539    0.004467   \n",
            "\n",
            "     var1(t+49)  var2(t+49)  var3(t+49)  var4(t+49)  \n",
            "175    1.530042   -0.000232   -0.032110    0.003410  \n",
            "176    1.391068   -0.032342   -0.028700    0.003905  \n",
            "177    3.933360   -0.061043   -0.024795    0.004257  \n",
            "178    5.136263   -0.085838   -0.020539    0.004467  \n",
            "179    3.502504   -0.106376   -0.016072    0.004541  \n",
            "\n",
            "[5 rows x 378 columns]\n",
            "Index(['var1(t-175)', 'var1(t-174)', 'var1(t-173)', 'var1(t-172)',\n",
            "       'var1(t-171)', 'var1(t-170)', 'var1(t-169)', 'var1(t-168)',\n",
            "       'var1(t-167)', 'var1(t-166)',\n",
            "       ...\n",
            "       'var3(t+47)', 'var4(t+47)', 'var1(t+48)', 'var2(t+48)', 'var3(t+48)',\n",
            "       'var4(t+48)', 'var1(t+49)', 'var2(t+49)', 'var3(t+49)', 'var4(t+49)'],\n",
            "      dtype='object', length=378)\n"
          ]
        }
      ],
      "source": [
        "data = Supervised(df.values, n_in = 175, n_out = 50)\n",
        "\n",
        "\n",
        "cols_to_drop = []\n",
        "for i in range(2, 176):\n",
        "    cols_to_drop.extend([f'var2(t-{i})', f'var3(t-{i})', f'var4(t-{i})'])\n",
        "\n",
        "data.drop(cols_to_drop, axis=1, inplace=True)\n",
        "\n",
        "print(data.head())\n",
        "print(data.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "AfPf60oy6Pe4"
      },
      "outputs": [],
      "source": [
        "train = np.array(data[0:len(data)-1])\n",
        "forecast = np.array(data.tail(1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "WSAafzI37KiT"
      },
      "outputs": [],
      "source": [
        "trainy = train[:,-150:]\n",
        "trainX = train[:,:-150]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "2SrOqVJA7f50"
      },
      "outputs": [],
      "source": [
        "forecasty = forecast[:,-150:]\n",
        "forecastX = forecast[:,:-150]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qno_k8Nw7saY",
        "outputId": "c4a88db5-d8c6-489f-cdb2-06b24e293cff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1739, 1, 228) (1739, 150) (1, 1, 228)\n"
          ]
        }
      ],
      "source": [
        "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
        "forecastX = forecastX.reshape((forecastX.shape[0], 1, forecastX.shape[1]))\n",
        "print(trainX.shape, trainy.shape, forecastX.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1Jp2DvNuNFx",
        "outputId": "d0e5b3c4-64f1-438c-eade-8dfeaac8e285"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "22/22 [==============================] - 13s 112ms/step - loss: 9795629.0000 - val_loss: 9793123.0000\n",
            "Epoch 2/500\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 9781386.0000 - val_loss: 9783019.0000\n",
            "Epoch 3/500\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 9764426.0000 - val_loss: 9768682.0000\n",
            "Epoch 4/500\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 9745370.0000 - val_loss: 9751629.0000\n",
            "Epoch 5/500\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 9726681.0000 - val_loss: 9733842.0000\n",
            "Epoch 6/500\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 9709108.0000 - val_loss: 9716103.0000\n",
            "Epoch 7/500\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 9692213.0000 - val_loss: 9698800.0000\n",
            "Epoch 8/500\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 9676006.0000 - val_loss: 9682068.0000\n",
            "Epoch 9/500\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 9660418.0000 - val_loss: 9665907.0000\n",
            "Epoch 10/500\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 9645356.0000 - val_loss: 9650267.0000\n",
            "Epoch 11/500\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 9630735.0000 - val_loss: 9635082.0000\n",
            "Epoch 12/500\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 9616477.0000 - val_loss: 9620290.0000\n",
            "Epoch 13/500\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 9602532.0000 - val_loss: 9605838.0000\n",
            "Epoch 14/500\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 9588848.0000 - val_loss: 9591683.0000\n",
            "Epoch 15/500\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 9575389.0000 - val_loss: 9577786.0000\n",
            "Epoch 16/500\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 9562127.0000 - val_loss: 9564115.0000\n",
            "Epoch 17/500\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 9549034.0000 - val_loss: 9550645.0000\n",
            "Epoch 18/500\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 9536095.0000 - val_loss: 9537352.0000\n",
            "Epoch 19/500\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 9523288.0000 - val_loss: 9524220.0000\n",
            "Epoch 20/500\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 9510601.0000 - val_loss: 9511231.0000\n",
            "Epoch 21/500\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 9498024.0000 - val_loss: 9498371.0000\n",
            "Epoch 22/500\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 9485547.0000 - val_loss: 9485629.0000\n",
            "Epoch 23/500\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 9473157.0000 - val_loss: 9472995.0000\n",
            "Epoch 24/500\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 9460852.0000 - val_loss: 9460459.0000\n",
            "Epoch 25/500\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 9448620.0000 - val_loss: 9448013.0000\n",
            "Epoch 26/500\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 9436461.0000 - val_loss: 9435652.0000\n",
            "Epoch 27/500\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 9424366.0000 - val_loss: 9423367.0000\n",
            "Epoch 28/500\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 9412331.0000 - val_loss: 9411156.0000\n",
            "Epoch 29/500\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 9400353.0000 - val_loss: 9399011.0000\n",
            "Epoch 30/500\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 9388427.0000 - val_loss: 9386929.0000\n",
            "Epoch 31/500\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 9376552.0000 - val_loss: 9374906.0000\n",
            "Epoch 32/500\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 9364724.0000 - val_loss: 9362938.0000\n",
            "Epoch 33/500\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 9352941.0000 - val_loss: 9351022.0000\n",
            "Epoch 34/500\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 9341200.0000 - val_loss: 9339156.0000\n",
            "Epoch 35/500\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 9329499.0000 - val_loss: 9327336.0000\n",
            "Epoch 36/500\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 9317836.0000 - val_loss: 9315560.0000\n",
            "Epoch 37/500\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 9306208.0000 - val_loss: 9303827.0000\n",
            "Epoch 38/500\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 9294618.0000 - val_loss: 9292134.0000\n",
            "Epoch 39/500\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 9283057.0000 - val_loss: 9280479.0000\n",
            "Epoch 40/500\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 9271531.0000 - val_loss: 9268860.0000\n",
            "Epoch 41/500\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 9260034.0000 - val_loss: 9257276.0000\n",
            "Epoch 42/500\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 9248566.0000 - val_loss: 9245725.0000\n",
            "Epoch 43/500\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 9237128.0000 - val_loss: 9234207.0000\n",
            "Epoch 44/500\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 9225715.0000 - val_loss: 9222719.0000\n",
            "Epoch 45/500\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 9214328.0000 - val_loss: 9211261.0000\n",
            "Epoch 46/500\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 9202968.0000 - val_loss: 9199831.0000\n",
            "Epoch 47/500\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 9191632.0000 - val_loss: 9188429.0000\n",
            "Epoch 48/500\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 9180320.0000 - val_loss: 9177053.0000\n",
            "Epoch 49/500\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 9169032.0000 - val_loss: 9165703.0000\n",
            "Epoch 50/500\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 9157764.0000 - val_loss: 9154380.0000\n",
            "Epoch 51/500\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 9146520.0000 - val_loss: 9143079.0000\n",
            "Epoch 52/500\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 9135295.0000 - val_loss: 9131801.0000\n",
            "Epoch 53/500\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 9124093.0000 - val_loss: 9120547.0000\n",
            "Epoch 54/500\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 9112909.0000 - val_loss: 9109315.0000\n",
            "Epoch 55/500\n",
            "22/22 [==============================] - 1s 32ms/step - loss: 9101746.0000 - val_loss: 9098105.0000\n",
            "Epoch 56/500\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 9090600.0000 - val_loss: 9086914.0000\n",
            "Epoch 57/500\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 9079475.0000 - val_loss: 9075744.0000\n",
            "Epoch 58/500\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 9068367.0000 - val_loss: 9064594.0000\n",
            "Epoch 59/500\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 9057277.0000 - val_loss: 9053464.0000\n",
            "Epoch 60/500\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 9046204.0000 - val_loss: 9042353.0000\n",
            "Epoch 61/500\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 9035148.0000 - val_loss: 9031259.0000\n",
            "Epoch 62/500\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 9024109.0000 - val_loss: 9020184.0000\n",
            "Epoch 63/500\n",
            "22/22 [==============================] - 1s 39ms/step - loss: 9013086.0000 - val_loss: 9009126.0000\n",
            "Epoch 64/500\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 9002078.0000 - val_loss: 8998086.0000\n",
            "Epoch 65/500\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 8991089.0000 - val_loss: 8987063.0000\n",
            "Epoch 66/500\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 8980112.0000 - val_loss: 8976056.0000\n",
            "Epoch 67/500\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 8969151.0000 - val_loss: 8965065.0000\n",
            "Epoch 68/500\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 8958206.0000 - val_loss: 8954091.0000\n",
            "Epoch 69/500\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 8947274.0000 - val_loss: 8943131.0000\n",
            "Epoch 70/500\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 8936357.0000 - val_loss: 8932187.0000\n",
            "Epoch 71/500\n",
            "22/22 [==============================] - 1s 42ms/step - loss: 8925454.0000 - val_loss: 8921259.0000\n",
            "Epoch 72/500\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 8914566.0000 - val_loss: 8910346.0000\n",
            "Epoch 73/500\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 8903691.0000 - val_loss: 8899446.0000\n",
            "Epoch 74/500\n",
            "22/22 [==============================] - 1s 28ms/step - loss: 8892831.0000 - val_loss: 8888561.0000\n",
            "Epoch 75/500\n",
            "22/22 [==============================] - 0s 22ms/step - loss: 8881982.0000 - val_loss: 8877689.0000\n",
            "Epoch 76/500\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 8871146.0000 - val_loss: 8866832.0000\n",
            "Epoch 77/500\n",
            "22/22 [==============================] - 1s 38ms/step - loss: 8860325.0000 - val_loss: 8855987.0000\n",
            "Epoch 78/500\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 8849515.0000 - val_loss: 8845160.0000\n",
            "Epoch 79/500\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 8838717.0000 - val_loss: 8834342.0000\n",
            "Epoch 80/500\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 8827933.0000 - val_loss: 8823538.0000\n",
            "Epoch 81/500\n",
            "22/22 [==============================] - 0s 21ms/step - loss: 8817163.0000 - val_loss: 8812748.0000\n",
            "Epoch 82/500\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 8806402.0000 - val_loss: 8801969.0000\n",
            "Epoch 83/500\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 8795654.0000 - val_loss: 8791204.0000\n",
            "Epoch 84/500\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 8784917.0000 - val_loss: 8780451.0000\n",
            "Epoch 85/500\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 8774193.0000 - val_loss: 8769710.0000\n",
            "Epoch 86/500\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 8763481.0000 - val_loss: 8758982.0000\n",
            "Epoch 87/500\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 8752779.0000 - val_loss: 8748264.0000\n",
            "Epoch 88/500\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 8742089.0000 - val_loss: 8737559.0000\n",
            "Epoch 89/500\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 8731409.0000 - val_loss: 8726865.0000\n",
            "Epoch 90/500\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 8720741.0000 - val_loss: 8716183.0000\n",
            "Epoch 91/500\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 8710085.0000 - val_loss: 8705514.0000\n",
            "Epoch 92/500\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 8699439.0000 - val_loss: 8694854.0000\n",
            "Epoch 93/500\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 8688803.0000 - val_loss: 8684205.0000\n",
            "Epoch 94/500\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 8678178.0000 - val_loss: 8673568.0000\n",
            "Epoch 95/500\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 8667564.0000 - val_loss: 8662942.0000\n",
            "Epoch 96/500\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 8656961.0000 - val_loss: 8652327.0000\n",
            "Epoch 97/500\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 8646367.0000 - val_loss: 8641721.0000\n",
            "Epoch 98/500\n",
            "22/22 [==============================] - 0s 20ms/step - loss: 8635784.0000 - val_loss: 8631128.0000\n",
            "Epoch 99/500\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 8625210.0000 - val_loss: 8620545.0000\n",
            "Epoch 100/500\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 8614649.0000 - val_loss: 8609971.0000\n",
            "Epoch 101/500\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 8604096.0000 - val_loss: 8599409.0000\n",
            "Epoch 102/500\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 8593553.0000 - val_loss: 8588857.0000\n",
            "Epoch 103/500\n",
            "22/22 [==============================] - 0s 22ms/step - loss: 8583020.0000 - val_loss: 8578314.0000\n",
            "Epoch 104/500\n",
            "22/22 [==============================] - 0s 22ms/step - loss: 8572497.0000 - val_loss: 8567781.0000\n",
            "Epoch 105/500\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 8561984.0000 - val_loss: 8557259.0000\n",
            "Epoch 106/500\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 8551479.0000 - val_loss: 8546746.0000\n",
            "Epoch 107/500\n",
            "22/22 [==============================] - 0s 20ms/step - loss: 8540986.0000 - val_loss: 8536243.0000\n",
            "Epoch 108/500\n",
            "22/22 [==============================] - 1s 29ms/step - loss: 8530501.0000 - val_loss: 8525751.0000\n",
            "Epoch 109/500\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 8520025.0000 - val_loss: 8515268.0000\n",
            "Epoch 110/500\n",
            "22/22 [==============================] - 0s 23ms/step - loss: 8509560.0000 - val_loss: 8504794.0000\n",
            "Epoch 111/500\n",
            "22/22 [==============================] - 1s 45ms/step - loss: 8499104.0000 - val_loss: 8494329.0000\n",
            "Epoch 112/500\n",
            "22/22 [==============================] - 0s 22ms/step - loss: 8488655.0000 - val_loss: 8483876.0000\n",
            "Epoch 113/500\n",
            "22/22 [==============================] - 0s 21ms/step - loss: 8478216.0000 - val_loss: 8473431.0000\n",
            "Epoch 114/500\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 8467789.0000 - val_loss: 8462994.0000\n",
            "Epoch 115/500\n",
            "22/22 [==============================] - 1s 29ms/step - loss: 8457368.0000 - val_loss: 8452569.0000\n",
            "Epoch 116/500\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 8446958.0000 - val_loss: 8442152.0000\n",
            "Epoch 117/500\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 8436556.0000 - val_loss: 8431743.0000\n",
            "Epoch 118/500\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 8426162.0000 - val_loss: 8421344.0000\n",
            "Epoch 119/500\n",
            "22/22 [==============================] - 1s 30ms/step - loss: 8415778.0000 - val_loss: 8410954.0000\n",
            "Epoch 120/500\n",
            "22/22 [==============================] - 0s 21ms/step - loss: 8405404.0000 - val_loss: 8400572.0000\n",
            "Epoch 121/500\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 8395036.0000 - val_loss: 8390200.0000\n",
            "Epoch 122/500\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 8384677.0000 - val_loss: 8379838.0000\n",
            "Epoch 123/500\n",
            "22/22 [==============================] - 0s 20ms/step - loss: 8374328.5000 - val_loss: 8369483.0000\n",
            "Epoch 124/500\n",
            "22/22 [==============================] - 1s 27ms/step - loss: 8363988.5000 - val_loss: 8359138.0000\n",
            "Epoch 125/500\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 8353656.0000 - val_loss: 8348800.5000\n",
            "Epoch 126/500\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 8343332.0000 - val_loss: 8338472.5000\n",
            "Epoch 127/500\n",
            "22/22 [==============================] - 1s 30ms/step - loss: 8333017.5000 - val_loss: 8328153.0000\n",
            "Epoch 128/500\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 8322710.5000 - val_loss: 8317841.5000\n",
            "Epoch 129/500\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 8312412.5000 - val_loss: 8307539.0000\n",
            "Epoch 130/500\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 8302122.5000 - val_loss: 8297244.5000\n",
            "Epoch 131/500\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 8291841.0000 - val_loss: 8286960.0000\n",
            "Epoch 132/500\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 8281568.0000 - val_loss: 8276683.0000\n",
            "Epoch 133/500\n",
            "22/22 [==============================] - 0s 20ms/step - loss: 8271301.5000 - val_loss: 8266414.5000\n",
            "Epoch 134/500\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 8261044.5000 - val_loss: 8256154.5000\n",
            "Epoch 135/500\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 8250796.5000 - val_loss: 8245902.0000\n",
            "Epoch 136/500\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 8240555.5000 - val_loss: 8235657.5000\n",
            "Epoch 137/500\n",
            "22/22 [==============================] - 0s 20ms/step - loss: 8230324.0000 - val_loss: 8225422.0000\n",
            "Epoch 138/500\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 8220100.0000 - val_loss: 8215195.0000\n",
            "Epoch 139/500\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 8209883.0000 - val_loss: 8204976.0000\n",
            "Epoch 140/500\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 8199674.5000 - val_loss: 8194764.5000\n",
            "Epoch 141/500\n",
            "22/22 [==============================] - 0s 21ms/step - loss: 8189475.0000 - val_loss: 8184562.0000\n",
            "Epoch 142/500\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 8179281.5000 - val_loss: 8174367.0000\n",
            "Epoch 143/500\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 8169096.5000 - val_loss: 8164180.0000\n",
            "Epoch 144/500\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 8158921.5000 - val_loss: 8154001.0000\n",
            "Epoch 145/500\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 8148751.5000 - val_loss: 8143830.0000\n",
            "Epoch 146/500\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 8138591.5000 - val_loss: 8133667.5000\n",
            "Epoch 147/500\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 8128440.0000 - val_loss: 8123512.5000\n",
            "Epoch 148/500\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 8118293.5000 - val_loss: 8113364.5000\n",
            "Epoch 149/500\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 8108156.5000 - val_loss: 8103225.5000\n",
            "Epoch 150/500\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 8098026.0000 - val_loss: 8093093.5000\n",
            "Epoch 151/500\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 8087904.5000 - val_loss: 8082969.5000\n",
            "Epoch 152/500\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 8077790.0000 - val_loss: 8072855.0000\n",
            "Epoch 153/500\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 8067683.5000 - val_loss: 8062745.5000\n",
            "Epoch 154/500\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 8057584.5000 - val_loss: 8052645.0000\n",
            "Epoch 155/500\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 8047493.0000 - val_loss: 8042552.5000\n",
            "Epoch 156/500\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 8037408.5000 - val_loss: 8032467.0000\n",
            "Epoch 157/500\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 8027332.0000 - val_loss: 8022389.0000\n",
            "Epoch 158/500\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 8017262.5000 - val_loss: 8012319.5000\n",
            "Epoch 159/500\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 8007203.0000 - val_loss: 8002257.5000\n",
            "Epoch 160/500\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 7997147.0000 - val_loss: 7992202.5000\n",
            "Epoch 161/500\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 7987103.0000 - val_loss: 7982155.0000\n",
            "Epoch 162/500\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 7977063.0000 - val_loss: 7972115.0000\n",
            "Epoch 163/500\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 7967031.5000 - val_loss: 7962082.5000\n",
            "Epoch 164/500\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 7957008.0000 - val_loss: 7952058.0000\n",
            "Epoch 165/500\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 7946991.5000 - val_loss: 7942041.0000\n",
            "Epoch 166/500\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 7936982.0000 - val_loss: 7932031.5000\n",
            "Epoch 167/500\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 7926980.5000 - val_loss: 7922029.0000\n",
            "Epoch 168/500\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 7916985.5000 - val_loss: 7912034.0000\n",
            "Epoch 169/500\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 7906999.0000 - val_loss: 7902046.0000\n",
            "Epoch 170/500\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 7897018.0000 - val_loss: 7892066.0000\n",
            "Epoch 171/500\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 7887046.0000 - val_loss: 7882093.0000\n",
            "Epoch 172/500\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 7877081.5000 - val_loss: 7872127.5000\n",
            "Epoch 173/500\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 7867123.5000 - val_loss: 7862169.5000\n",
            "Epoch 174/500\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 7857172.0000 - val_loss: 7852218.0000\n",
            "Epoch 175/500\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 7847229.5000 - val_loss: 7842274.5000\n",
            "Epoch 176/500\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 7837292.5000 - val_loss: 7832339.0000\n",
            "Epoch 177/500\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 7827363.0000 - val_loss: 7822409.5000\n",
            "Epoch 178/500\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 7817441.5000 - val_loss: 7812487.5000\n",
            "Epoch 179/500\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 7807526.5000 - val_loss: 7802572.5000\n",
            "Epoch 180/500\n",
            "22/22 [==============================] - 0s 22ms/step - loss: 7797619.5000 - val_loss: 7792665.0000\n",
            "Epoch 181/500\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 7787719.0000 - val_loss: 7782765.0000\n",
            "Epoch 182/500\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 7777824.0000 - val_loss: 7772872.0000\n",
            "Epoch 183/500\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 7767940.5000 - val_loss: 7762986.0000\n",
            "Epoch 184/500\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 7758059.5000 - val_loss: 7753107.0000\n",
            "Epoch 185/500\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 7748189.0000 - val_loss: 7743234.0000\n",
            "Epoch 186/500\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 7738322.5000 - val_loss: 7733370.0000\n",
            "Epoch 187/500\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 7728465.0000 - val_loss: 7723512.5000\n",
            "Epoch 188/500\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 7718613.0000 - val_loss: 7713662.5000\n",
            "Epoch 189/500\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 7708768.5000 - val_loss: 7703819.0000\n",
            "Epoch 190/500\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 7698934.0000 - val_loss: 7693983.0000\n",
            "Epoch 191/500\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 7689104.0000 - val_loss: 7684155.0000\n",
            "Epoch 192/500\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 7679281.5000 - val_loss: 7674332.0000\n",
            "Epoch 193/500\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 7669465.0000 - val_loss: 7664516.5000\n",
            "Epoch 194/500\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 7659656.0000 - val_loss: 7654708.0000\n",
            "Epoch 195/500\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 7649855.5000 - val_loss: 7644908.0000\n",
            "Epoch 196/500\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 7640059.5000 - val_loss: 7635113.0000\n",
            "Epoch 197/500\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 7630270.0000 - val_loss: 7625326.5000\n",
            "Epoch 198/500\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 7620490.0000 - val_loss: 7615546.0000\n",
            "Epoch 199/500\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 7610715.5000 - val_loss: 7605773.0000\n",
            "Epoch 200/500\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 7600949.0000 - val_loss: 7596007.0000\n",
            "Epoch 201/500\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 7591189.5000 - val_loss: 7586247.5000\n",
            "Epoch 202/500\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 7581436.0000 - val_loss: 7576495.0000\n",
            "Epoch 203/500\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 7571689.5000 - val_loss: 7566750.0000\n",
            "Epoch 204/500\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 7561950.5000 - val_loss: 7557011.0000\n",
            "Epoch 205/500\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 7552218.0000 - val_loss: 7547279.5000\n",
            "Epoch 206/500\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 7542491.5000 - val_loss: 7537555.0000\n",
            "Epoch 207/500\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 7532773.0000 - val_loss: 7527837.5000\n",
            "Epoch 208/500\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 7523062.0000 - val_loss: 7518126.5000\n",
            "Epoch 209/500\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 7513355.0000 - val_loss: 7508422.5000\n",
            "Epoch 210/500\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 7503656.5000 - val_loss: 7498725.5000\n",
            "Epoch 211/500\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 7493964.5000 - val_loss: 7489034.5000\n",
            "Epoch 212/500\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 7484281.5000 - val_loss: 7479352.0000\n",
            "Epoch 213/500\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 7474601.5000 - val_loss: 7469675.5000\n",
            "Epoch 214/500\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 7464930.5000 - val_loss: 7460005.5000\n",
            "Epoch 215/500\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 7455267.5000 - val_loss: 7450342.5000\n",
            "Epoch 216/500\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 7445610.0000 - val_loss: 7440686.5000\n",
            "Epoch 217/500\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 7435959.0000 - val_loss: 7431037.0000\n",
            "Epoch 218/500\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 7426315.0000 - val_loss: 7421394.5000\n",
            "Epoch 219/500\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 7416677.5000 - val_loss: 7411758.5000\n",
            "Epoch 220/500\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 7407047.5000 - val_loss: 7402129.0000\n",
            "Epoch 221/500\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 7397424.0000 - val_loss: 7392507.5000\n",
            "Epoch 222/500\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 7387806.5000 - val_loss: 7382892.0000\n",
            "Epoch 223/500\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 7378197.5000 - val_loss: 7373283.5000\n",
            "Epoch 224/500\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 7368593.5000 - val_loss: 7363682.0000\n",
            "Epoch 225/500\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 7358997.0000 - val_loss: 7354086.5000\n",
            "Epoch 226/500\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 7349407.0000 - val_loss: 7344498.0000\n",
            "Epoch 227/500\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 7339823.0000 - val_loss: 7334916.5000\n",
            "Epoch 228/500\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 7330247.0000 - val_loss: 7325340.5000\n",
            "Epoch 229/500\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 7320677.0000 - val_loss: 7315773.0000\n",
            "Epoch 230/500\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 7311112.5000 - val_loss: 7306211.5000\n",
            "Epoch 231/500\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 7301554.5000 - val_loss: 7296657.0000\n",
            "Epoch 232/500\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 7292007.0000 - val_loss: 7287109.0000\n",
            "Epoch 233/500\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 7282464.0000 - val_loss: 7277568.0000\n",
            "Epoch 234/500\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 7272928.0000 - val_loss: 7268033.5000\n",
            "Epoch 235/500\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 7263397.5000 - val_loss: 7258505.0000\n",
            "Epoch 236/500\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 7253875.5000 - val_loss: 7248983.0000\n",
            "Epoch 237/500\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 7244357.5000 - val_loss: 7239469.0000\n",
            "Epoch 238/500\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 7234847.0000 - val_loss: 7229961.0000\n",
            "Epoch 239/500\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 7225344.0000 - val_loss: 7220458.0000\n",
            "Epoch 240/500\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 7215846.0000 - val_loss: 7210964.0000\n",
            "Epoch 241/500\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 7206357.0000 - val_loss: 7201475.5000\n",
            "Epoch 242/500\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 7196874.5000 - val_loss: 7191995.5000\n",
            "Epoch 243/500\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 7187398.0000 - val_loss: 7182522.0000\n",
            "Epoch 244/500\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 7177929.5000 - val_loss: 7173054.0000\n",
            "Epoch 245/500\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 7168466.0000 - val_loss: 7163593.5000\n",
            "Epoch 246/500\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 7159009.5000 - val_loss: 7154138.5000\n",
            "Epoch 247/500\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 7149560.5000 - val_loss: 7144690.0000\n",
            "Epoch 248/500\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 7140116.5000 - val_loss: 7135248.0000\n",
            "Epoch 249/500\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 7130678.0000 - val_loss: 7125812.0000\n",
            "Epoch 250/500\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 7121247.0000 - val_loss: 7116383.5000\n",
            "Epoch 251/500\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 7111823.5000 - val_loss: 7106961.5000\n",
            "Epoch 252/500\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 7102405.0000 - val_loss: 7097546.5000\n",
            "Epoch 253/500\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 7092995.0000 - val_loss: 7088138.5000\n",
            "Epoch 254/500\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 7083589.5000 - val_loss: 7078737.0000\n",
            "Epoch 255/500\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 7074194.5000 - val_loss: 7069343.0000\n",
            "Epoch 256/500\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 7064804.0000 - val_loss: 7059955.0000\n",
            "Epoch 257/500\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 7055421.0000 - val_loss: 7050573.0000\n",
            "Epoch 258/500\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 7046045.5000 - val_loss: 7041199.0000\n",
            "Epoch 259/500\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 7036674.0000 - val_loss: 7031831.0000\n",
            "Epoch 260/500\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 7027309.5000 - val_loss: 7022468.5000\n",
            "Epoch 261/500\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 7017952.0000 - val_loss: 7013112.0000\n",
            "Epoch 262/500\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 7008600.5000 - val_loss: 7003763.0000\n",
            "Epoch 263/500\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 6999253.5000 - val_loss: 6994419.0000\n",
            "Epoch 264/500\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 6989915.5000 - val_loss: 6985082.0000\n",
            "Epoch 265/500\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 6980581.5000 - val_loss: 6975752.0000\n",
            "Epoch 266/500\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 6971256.5000 - val_loss: 6966428.5000\n",
            "Epoch 267/500\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 6961938.0000 - val_loss: 6957112.5000\n",
            "Epoch 268/500\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 6952627.5000 - val_loss: 6947803.0000\n",
            "Epoch 269/500\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 6943322.0000 - val_loss: 6938502.5000\n",
            "Epoch 270/500\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 6934023.5000 - val_loss: 6929206.5000\n",
            "Epoch 271/500\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 6924733.5000 - val_loss: 6919917.5000\n",
            "Epoch 272/500\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 6915448.0000 - val_loss: 6910634.5000\n",
            "Epoch 273/500\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 6906169.0000 - val_loss: 6901357.5000\n",
            "Epoch 274/500\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 6896897.0000 - val_loss: 6892088.0000\n",
            "Epoch 275/500\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 6887630.0000 - val_loss: 6882823.5000\n",
            "Epoch 276/500\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 6878370.5000 - val_loss: 6873566.0000\n",
            "Epoch 277/500\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 6869116.5000 - val_loss: 6864314.0000\n",
            "Epoch 278/500\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 6859869.5000 - val_loss: 6855069.5000\n",
            "Epoch 279/500\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 6850627.5000 - val_loss: 6845830.5000\n",
            "Epoch 280/500\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 6841392.5000 - val_loss: 6836598.5000\n",
            "Epoch 281/500\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 6832165.0000 - val_loss: 6827373.0000\n",
            "Epoch 282/500\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 6822943.5000 - val_loss: 6818154.5000\n",
            "Epoch 283/500\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 6813729.0000 - val_loss: 6808943.0000\n",
            "Epoch 284/500\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 6804522.5000 - val_loss: 6799739.5000\n",
            "Epoch 285/500\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 6795323.0000 - val_loss: 6790542.5000\n",
            "Epoch 286/500\n",
            "22/22 [==============================] - 0s 23ms/step - loss: 6786129.5000 - val_loss: 6781351.0000\n",
            "Epoch 287/500\n",
            "22/22 [==============================] - 1s 39ms/step - loss: 6776942.5000 - val_loss: 6772168.0000\n",
            "Epoch 288/500\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 6767762.0000 - val_loss: 6762989.0000\n",
            "Epoch 289/500\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 6758587.5000 - val_loss: 6753816.5000\n",
            "Epoch 290/500\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 6749419.0000 - val_loss: 6744651.0000\n",
            "Epoch 291/500\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 6740258.0000 - val_loss: 6735491.0000\n",
            "Epoch 292/500\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 6731101.5000 - val_loss: 6726337.5000\n",
            "Epoch 293/500\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 6721952.0000 - val_loss: 6717190.5000\n",
            "Epoch 294/500\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 6712807.5000 - val_loss: 6708049.0000\n",
            "Epoch 295/500\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 6703671.0000 - val_loss: 6698914.5000\n",
            "Epoch 296/500\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 6694540.0000 - val_loss: 6689787.0000\n",
            "Epoch 297/500\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 6685415.5000 - val_loss: 6680664.5000\n",
            "Epoch 298/500\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 6676297.5000 - val_loss: 6671549.0000\n",
            "Epoch 299/500\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 6667186.5000 - val_loss: 6662442.0000\n",
            "Epoch 300/500\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 6658083.5000 - val_loss: 6653342.0000\n",
            "Epoch 301/500\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 6648986.5000 - val_loss: 6644248.5000\n",
            "Epoch 302/500\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 6639898.0000 - val_loss: 6635162.5000\n",
            "Epoch 303/500\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 6630816.5000 - val_loss: 6626081.0000\n",
            "Epoch 304/500\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 6621738.5000 - val_loss: 6617007.0000\n",
            "Epoch 305/500\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 6612668.0000 - val_loss: 6607939.5000\n",
            "Epoch 306/500\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 6603603.5000 - val_loss: 6598878.0000\n",
            "Epoch 307/500\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 6594545.5000 - val_loss: 6589822.5000\n",
            "Epoch 308/500\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 6585493.5000 - val_loss: 6580773.0000\n",
            "Epoch 309/500\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 6576449.0000 - val_loss: 6571730.5000\n",
            "Epoch 310/500\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 6567408.0000 - val_loss: 6562693.0000\n",
            "Epoch 311/500\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 6558376.0000 - val_loss: 6553662.5000\n",
            "Epoch 312/500\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 6549348.5000 - val_loss: 6544637.0000\n",
            "Epoch 313/500\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 6540327.0000 - val_loss: 6535619.5000\n",
            "Epoch 314/500\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 6531313.5000 - val_loss: 6526608.0000\n",
            "Epoch 315/500\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 6522305.0000 - val_loss: 6517602.5000\n",
            "Epoch 316/500\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 6513303.5000 - val_loss: 6508603.5000\n",
            "Epoch 317/500\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 6504308.0000 - val_loss: 6499613.5000\n",
            "Epoch 318/500\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 6495321.0000 - val_loss: 6490629.0000\n",
            "Epoch 319/500\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 6486342.0000 - val_loss: 6481652.0000\n",
            "Epoch 320/500\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 6477370.0000 - val_loss: 6472682.5000\n",
            "Epoch 321/500\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 6468402.5000 - val_loss: 6463718.5000\n",
            "Epoch 322/500\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 6459442.0000 - val_loss: 6454761.0000\n",
            "Epoch 323/500\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 6450488.0000 - val_loss: 6445809.5000\n",
            "Epoch 324/500\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 6441540.0000 - val_loss: 6436864.0000\n",
            "Epoch 325/500\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 6432598.0000 - val_loss: 6427925.5000\n",
            "Epoch 326/500\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 6423662.5000 - val_loss: 6418992.0000\n",
            "Epoch 327/500\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 6414733.5000 - val_loss: 6410066.5000\n",
            "Epoch 328/500\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 6405810.5000 - val_loss: 6401145.5000\n",
            "Epoch 329/500\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 6396893.5000 - val_loss: 6392231.0000\n",
            "Epoch 330/500\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 6387982.0000 - val_loss: 6383324.0000\n",
            "Epoch 331/500\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 6379079.0000 - val_loss: 6374421.5000\n",
            "Epoch 332/500\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 6370179.5000 - val_loss: 6365526.0000\n",
            "Epoch 333/500\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 6361288.0000 - val_loss: 6356636.5000\n",
            "Epoch 334/500\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 6352403.0000 - val_loss: 6347754.5000\n",
            "Epoch 335/500\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 6343522.5000 - val_loss: 6338877.0000\n",
            "Epoch 336/500\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 6334650.0000 - val_loss: 6330008.5000\n",
            "Epoch 337/500\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 6325785.5000 - val_loss: 6321147.0000\n",
            "Epoch 338/500\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 6316928.0000 - val_loss: 6312293.0000\n",
            "Epoch 339/500\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 6308078.0000 - val_loss: 6303445.5000\n",
            "Epoch 340/500\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 6299233.5000 - val_loss: 6294604.5000\n",
            "Epoch 341/500\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 6290395.0000 - val_loss: 6285769.0000\n",
            "Epoch 342/500\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 6281564.0000 - val_loss: 6276940.0000\n",
            "Epoch 343/500\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 6272738.5000 - val_loss: 6268117.5000\n",
            "Epoch 344/500\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 6263919.0000 - val_loss: 6259301.0000\n",
            "Epoch 345/500\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 6255105.0000 - val_loss: 6250490.0000\n",
            "Epoch 346/500\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 6246299.0000 - val_loss: 6241686.0000\n",
            "Epoch 347/500\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 6237498.5000 - val_loss: 6232888.5000\n",
            "Epoch 348/500\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 6228703.5000 - val_loss: 6224097.0000\n",
            "Epoch 349/500\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 6219915.0000 - val_loss: 6215311.5000\n",
            "Epoch 350/500\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 6211132.5000 - val_loss: 6206531.5000\n",
            "Epoch 351/500\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 6202357.0000 - val_loss: 6197758.5000\n",
            "Epoch 352/500\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 6193587.0000 - val_loss: 6188992.0000\n",
            "Epoch 353/500\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 6184824.5000 - val_loss: 6180231.5000\n",
            "Epoch 354/500\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 6176066.5000 - val_loss: 6171476.5000\n",
            "Epoch 355/500\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 6167315.5000 - val_loss: 6162728.0000\n",
            "Epoch 356/500\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 6158571.5000 - val_loss: 6153987.5000\n",
            "Epoch 357/500\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 6149834.0000 - val_loss: 6145255.0000\n",
            "Epoch 358/500\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 6141104.5000 - val_loss: 6136529.0000\n",
            "Epoch 359/500\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 6132381.0000 - val_loss: 6127807.5000\n",
            "Epoch 360/500\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 6123664.5000 - val_loss: 6119092.5000\n",
            "Epoch 361/500\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 6114951.5000 - val_loss: 6110384.5000\n",
            "Epoch 362/500\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 6106246.5000 - val_loss: 6101681.5000\n",
            "Epoch 363/500\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 6097547.5000 - val_loss: 6092984.5000\n",
            "Epoch 364/500\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 6088854.0000 - val_loss: 6084294.5000\n",
            "Epoch 365/500\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 6080166.5000 - val_loss: 6075609.5000\n",
            "Epoch 366/500\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 6071486.5000 - val_loss: 6066932.5000\n",
            "Epoch 367/500\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 6062814.0000 - val_loss: 6058265.5000\n",
            "Epoch 368/500\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 6054150.5000 - val_loss: 6049605.0000\n",
            "Epoch 369/500\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 6045493.5000 - val_loss: 6040951.0000\n",
            "Epoch 370/500\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 6036842.0000 - val_loss: 6032303.5000\n",
            "Epoch 371/500\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 6028199.0000 - val_loss: 6023663.0000\n",
            "Epoch 372/500\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 6019561.0000 - val_loss: 6015028.0000\n",
            "Epoch 373/500\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 6010929.5000 - val_loss: 6006399.0000\n",
            "Epoch 374/500\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 6002304.5000 - val_loss: 5997777.5000\n",
            "Epoch 375/500\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 5993686.5000 - val_loss: 5989161.5000\n",
            "Epoch 376/500\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 5985072.0000 - val_loss: 5980551.5000\n",
            "Epoch 377/500\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 5976466.5000 - val_loss: 5971948.5000\n",
            "Epoch 378/500\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 5967867.0000 - val_loss: 5963351.5000\n",
            "Epoch 379/500\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 5959273.5000 - val_loss: 5954761.0000\n",
            "Epoch 380/500\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 5950685.5000 - val_loss: 5946176.5000\n",
            "Epoch 381/500\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 5942104.0000 - val_loss: 5937598.0000\n",
            "Epoch 382/500\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 5933528.5000 - val_loss: 5929026.5000\n",
            "Epoch 383/500\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 5924959.5000 - val_loss: 5920459.5000\n",
            "Epoch 384/500\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 5916396.5000 - val_loss: 5911900.5000\n",
            "Epoch 385/500\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 5907840.5000 - val_loss: 5903345.5000\n",
            "Epoch 386/500\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 5899290.0000 - val_loss: 5894798.5000\n",
            "Epoch 387/500\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 5890745.5000 - val_loss: 5886257.0000\n",
            "Epoch 388/500\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 5882207.0000 - val_loss: 5877722.0000\n",
            "Epoch 389/500\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 5873676.0000 - val_loss: 5869193.5000\n",
            "Epoch 390/500\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 5865150.0000 - val_loss: 5860671.0000\n",
            "Epoch 391/500\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 5856630.5000 - val_loss: 5852155.0000\n",
            "Epoch 392/500\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 5848117.0000 - val_loss: 5843644.5000\n",
            "Epoch 393/500\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 5839610.0000 - val_loss: 5835140.5000\n",
            "Epoch 394/500\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 5831109.0000 - val_loss: 5826642.0000\n",
            "Epoch 395/500\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 5822614.5000 - val_loss: 5818150.5000\n",
            "Epoch 396/500\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 5814126.0000 - val_loss: 5809665.5000\n",
            "Epoch 397/500\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 5805644.0000 - val_loss: 5801186.0000\n",
            "Epoch 398/500\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 5797168.0000 - val_loss: 5792712.5000\n",
            "Epoch 399/500\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 5788698.0000 - val_loss: 5784245.5000\n",
            "Epoch 400/500\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 5780234.0000 - val_loss: 5775785.0000\n",
            "Epoch 401/500\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 5771777.0000 - val_loss: 5767331.0000\n",
            "Epoch 402/500\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 5763325.0000 - val_loss: 5758883.0000\n",
            "Epoch 403/500\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 5754879.5000 - val_loss: 5750439.5000\n",
            "Epoch 404/500\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 5746441.5000 - val_loss: 5742004.5000\n",
            "Epoch 405/500\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 5738008.0000 - val_loss: 5733574.0000\n",
            "Epoch 406/500\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 5729582.0000 - val_loss: 5725151.5000\n",
            "Epoch 407/500\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 5721165.0000 - val_loss: 5716741.0000\n",
            "Epoch 408/500\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 5712758.0000 - val_loss: 5708339.0000\n",
            "Epoch 409/500\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 5704358.0000 - val_loss: 5699941.5000\n",
            "Epoch 410/500\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 5695965.0000 - val_loss: 5691551.5000\n",
            "Epoch 411/500\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 5687577.0000 - val_loss: 5683166.5000\n",
            "Epoch 412/500\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 5679197.0000 - val_loss: 5674788.0000\n",
            "Epoch 413/500\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 5670822.0000 - val_loss: 5666416.0000\n",
            "Epoch 414/500\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 5662453.0000 - val_loss: 5658050.5000\n",
            "Epoch 415/500\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 5654091.0000 - val_loss: 5649691.0000\n",
            "Epoch 416/500\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 5645734.0000 - val_loss: 5641338.0000\n",
            "Epoch 417/500\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 5637383.0000 - val_loss: 5632991.0000\n",
            "Epoch 418/500\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 5629039.5000 - val_loss: 5624649.5000\n",
            "Epoch 419/500\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 5620701.0000 - val_loss: 5616315.5000\n",
            "Epoch 420/500\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 5612369.5000 - val_loss: 5607985.5000\n",
            "Epoch 421/500\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 5604044.5000 - val_loss: 5599664.0000\n",
            "Epoch 422/500\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 5595725.0000 - val_loss: 5591347.5000\n",
            "Epoch 423/500\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 5587412.0000 - val_loss: 5583037.0000\n",
            "Epoch 424/500\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 5579104.5000 - val_loss: 5574733.0000\n",
            "Epoch 425/500\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 5570803.5000 - val_loss: 5566435.5000\n",
            "Epoch 426/500\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 5562509.5000 - val_loss: 5558144.0000\n",
            "Epoch 427/500\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 5554221.0000 - val_loss: 5549858.0000\n",
            "Epoch 428/500\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 5545939.0000 - val_loss: 5541579.5000\n",
            "Epoch 429/500\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 5537662.5000 - val_loss: 5533306.0000\n",
            "Epoch 430/500\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 5529392.5000 - val_loss: 5525040.0000\n",
            "Epoch 431/500\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 5521128.5000 - val_loss: 5516779.0000\n",
            "Epoch 432/500\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 5512870.5000 - val_loss: 5508525.0000\n",
            "Epoch 433/500\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 5504619.5000 - val_loss: 5500276.5000\n",
            "Epoch 434/500\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 5496374.0000 - val_loss: 5492034.0000\n",
            "Epoch 435/500\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 5488135.0000 - val_loss: 5483797.5000\n",
            "Epoch 436/500\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 5479902.5000 - val_loss: 5475568.0000\n",
            "Epoch 437/500\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 5471675.5000 - val_loss: 5467344.5000\n",
            "Epoch 438/500\n",
            "22/22 [==============================] - 0s 21ms/step - loss: 5463455.5000 - val_loss: 5459127.0000\n",
            "Epoch 439/500\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 5455240.5000 - val_loss: 5450915.5000\n",
            "Epoch 440/500\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 5447032.5000 - val_loss: 5442710.5000\n",
            "Epoch 441/500\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 5438831.0000 - val_loss: 5434512.0000\n",
            "Epoch 442/500\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 5430635.0000 - val_loss: 5426320.0000\n",
            "Epoch 443/500\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 5422445.5000 - val_loss: 5418133.0000\n",
            "Epoch 444/500\n",
            "22/22 [==============================] - 0s 22ms/step - loss: 5414262.5000 - val_loss: 5409953.0000\n",
            "Epoch 445/500\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 5406085.0000 - val_loss: 5401779.0000\n",
            "Epoch 446/500\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 5397914.5000 - val_loss: 5393613.5000\n",
            "Epoch 447/500\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 5389755.5000 - val_loss: 5385460.0000\n",
            "Epoch 448/500\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 5381605.0000 - val_loss: 5377312.5000\n",
            "Epoch 449/500\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 5373460.5000 - val_loss: 5369172.0000\n",
            "Epoch 450/500\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 5365322.5000 - val_loss: 5361036.0000\n",
            "Epoch 451/500\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 5357190.0000 - val_loss: 5352907.0000\n",
            "Epoch 452/500\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 5349064.0000 - val_loss: 5344784.0000\n",
            "Epoch 453/500\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 5340944.5000 - val_loss: 5336667.0000\n",
            "Epoch 454/500\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 5332831.0000 - val_loss: 5328557.0000\n",
            "Epoch 455/500\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 5324724.0000 - val_loss: 5320452.5000\n",
            "Epoch 456/500\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 5316621.5000 - val_loss: 5312354.5000\n",
            "Epoch 457/500\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 5308527.0000 - val_loss: 5304263.0000\n",
            "Epoch 458/500\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 5300438.5000 - val_loss: 5296176.0000\n",
            "Epoch 459/500\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 5292355.0000 - val_loss: 5288097.0000\n",
            "Epoch 460/500\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 5284279.0000 - val_loss: 5280023.0000\n",
            "Epoch 461/500\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 5276207.5000 - val_loss: 5271956.0000\n",
            "Epoch 462/500\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 5268143.5000 - val_loss: 5263895.0000\n",
            "Epoch 463/500\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 5260085.5000 - val_loss: 5255839.5000\n",
            "Epoch 464/500\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 5252033.5000 - val_loss: 5247791.5000\n",
            "Epoch 465/500\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 5243988.0000 - val_loss: 5239748.5000\n",
            "Epoch 466/500\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 5235948.0000 - val_loss: 5231711.5000\n",
            "Epoch 467/500\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 5227914.5000 - val_loss: 5223681.5000\n",
            "Epoch 468/500\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 5219887.0000 - val_loss: 5215657.5000\n",
            "Epoch 469/500\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 5211866.5000 - val_loss: 5207640.0000\n",
            "Epoch 470/500\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 5203851.5000 - val_loss: 5199627.5000\n",
            "Epoch 471/500\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 5195842.5000 - val_loss: 5191622.5000\n",
            "Epoch 472/500\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 5187840.0000 - val_loss: 5183623.0000\n",
            "Epoch 473/500\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 5179844.0000 - val_loss: 5175629.0000\n",
            "Epoch 474/500\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 5171853.0000 - val_loss: 5167642.0000\n",
            "Epoch 475/500\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 5163869.0000 - val_loss: 5159661.0000\n",
            "Epoch 476/500\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 5155891.0000 - val_loss: 5151686.5000\n",
            "Epoch 477/500\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 5147919.5000 - val_loss: 5143718.5000\n",
            "Epoch 478/500\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 5139953.5000 - val_loss: 5135755.0000\n",
            "Epoch 479/500\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 5131994.5000 - val_loss: 5127799.0000\n",
            "Epoch 480/500\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 5124041.0000 - val_loss: 5119849.0000\n",
            "Epoch 481/500\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 5116094.0000 - val_loss: 5111905.5000\n",
            "Epoch 482/500\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 5108153.5000 - val_loss: 5103967.5000\n",
            "Epoch 483/500\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 5100217.5000 - val_loss: 5096035.5000\n",
            "Epoch 484/500\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 5092290.0000 - val_loss: 5088112.5000\n",
            "Epoch 485/500\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 5084372.5000 - val_loss: 5080201.5000\n",
            "Epoch 486/500\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 5076465.0000 - val_loss: 5072295.5000\n",
            "Epoch 487/500\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 5068562.5000 - val_loss: 5064397.0000\n",
            "Epoch 488/500\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 5060666.5000 - val_loss: 5056504.0000\n",
            "Epoch 489/500\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 5052776.5000 - val_loss: 5048617.0000\n",
            "Epoch 490/500\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 5044893.0000 - val_loss: 5040736.5000\n",
            "Epoch 491/500\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 5037015.0000 - val_loss: 5032862.0000\n",
            "Epoch 492/500\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 5029143.0000 - val_loss: 5024994.0000\n",
            "Epoch 493/500\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 5021278.0000 - val_loss: 5017132.0000\n",
            "Epoch 494/500\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 5013418.5000 - val_loss: 5009275.0000\n",
            "Epoch 495/500\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 5005566.0000 - val_loss: 5001425.0000\n",
            "Epoch 496/500\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 4997719.0000 - val_loss: 4993582.0000\n",
            "Epoch 497/500\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 4989878.5000 - val_loss: 4985744.0000\n",
            "Epoch 498/500\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 4982044.0000 - val_loss: 4977912.0000\n",
            "Epoch 499/500\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 4974214.5000 - val_loss: 4970087.5000\n",
            "Epoch 500/500\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 4966393.0000 - val_loss: 4962268.5000\n"
          ]
        }
      ],
      "source": [
        "c1 = tf.Variable(2.6676, name=\"c1\", trainable=True, dtype=tf.float32)\n",
        "c2 = tf.Variable(7000, name=\"c2\", trainable=True, dtype=tf.float32)\n",
        "c3 = tf.Variable(20000, name=\"c3\", trainable=True, dtype=tf.float32)\n",
        "b3 = tf.Variable(1.06, name=\"b3\", trainable=True, dtype=tf.float32)\n",
        "\n",
        "splitr = 0.8\n",
        "\n",
        "\n",
        "def loss_fn(y_true, y_pred):\n",
        "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
        "    #squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
        "    #squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
        "    squared_difference3 = tf.square(\n",
        "        y_pred[:, 3] + (c1 * y_pred[:, 1] + b3 * y_pred[:, 2] + c2))\n",
        "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
        "model = Sequential()\n",
        "model.add(LSTM(50, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
        "model.add(Dense(30))\n",
        "model.compile(loss=loss_fn, optimizer='adam')\n",
        "history = model.fit(trainX[:int(splitr*trainX.shape[0])], trainy[:int(splitr*trainX.shape[0])], epochs=500, batch_size=64, validation_data=(trainX[int(splitr*trainX.shape[0]):trainX.shape[0]], trainy[int(splitr*trainX.shape[0]):trainX.shape[0]]), shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJL101rPyuoT",
        "outputId": "239ff2b1-c186-423a-d316-939dfdd7c793"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 470ms/step\n"
          ]
        }
      ],
      "source": [
        "forecast_without_mc = forecastX\n",
        "yhat_without_mc = model.predict(forecast_without_mc) # Step Ahead Prediction\n",
        "forecast_without_mc = forecast_without_mc.reshape((forecast_without_mc.shape[0], forecast_without_mc.shape[2])) # Historical Input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9dQELcJ8wbp",
        "outputId": "4dc7671b-b1a8-48d5-8744-a86f98446109"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1, 1, 228)"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "forecastX.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IS2kyIKG1Kbr",
        "outputId": "f31b3485-8e26-452f-9298-ea8bf7e80ad1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1, 228)"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "forecast_without_mc.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "0u6VIzaDyuoT"
      },
      "outputs": [],
      "source": [
        "inv_yhat_without_mc = np.concatenate((forecast_without_mc, yhat_without_mc), axis=1) # Concatenation of predicted values with Historical Data\n",
        "#inv_yhat_without_mc = scaler.inverse_transform(inv_yhat_without_mc) # Transform labels back to original encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUEcw0LX07oU",
        "outputId": "cfc32397-9c37-4b43-d087-584bb31c3dcc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1, 258)"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "inv_yhat_without_mc.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "31OWVbSh_305"
      },
      "outputs": [],
      "source": [
        "fforecast = inv_yhat_without_mc[:,-150:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "BlpGH2FOAiRF"
      },
      "outputs": [],
      "source": [
        "final_forecast = fforecast[:,0:149:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "CXkgkj_LBk_t"
      },
      "outputs": [],
      "source": [
        "# code to replace all negative value with 0\n",
        "final_forecast[final_forecast<0] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[1.39290377e-01, 8.39560926e-02, 9.93205681e-02, 1.40223220e-01,\n",
              "        9.69569981e-02, 1.33461386e-01, 8.86839777e-02, 7.23788291e-02,\n",
              "        1.02643691e-01, 1.29834518e-01, 1.17524967e-01, 1.40378401e-01,\n",
              "        1.09736405e-01, 9.22384858e-02, 8.14174935e-02, 1.79841995e-01,\n",
              "        2.14529082e-01, 1.74751252e-01, 1.56199366e-01, 1.45695046e-01,\n",
              "        1.33179188e-01, 1.01875000e-01, 1.45557135e-01, 3.28982700e-03,\n",
              "        8.21864100e-03, 0.00000000e+00, 1.48621470e-01, 1.75536100e-03,\n",
              "        1.50761910e-02, 0.00000000e+00, 1.71634346e-01, 1.76133000e-04,\n",
              "        1.71493690e-02, 0.00000000e+00, 1.69606566e-01, 0.00000000e+00,\n",
              "        1.50379540e-02, 8.09951100e-03, 1.64701954e-01, 0.00000000e+00,\n",
              "        9.66649503e-04, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        2.25235343e-01, 0.00000000e+00, 0.00000000e+00, 1.65990382e-01,\n",
              "        2.60140151e-01, 3.42648923e-01]])"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "final_forecast"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "training_set = np.array(training_set)\n",
        "test = np.array(test)\n",
        "final_forecast = np.array(final_forecast.squeeze(0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.09953641675422191\n",
            "0.08963537706512904\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "MSE = np.square(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
        "rsme = math.sqrt(MSE)\n",
        "print(rsme)  \n",
        "MAE = np.abs(np.subtract(np.array(test),np.array(final_forecast))).mean()   \n",
        "mae = MAE\n",
        "print(mae)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
